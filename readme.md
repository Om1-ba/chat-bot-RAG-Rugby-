# ğŸ‰ RAG Rugby Chatbot

## ğŸŒ Application dÃ©ployÃ©e (via Cloudflared : mon pc doit donc etre allumÃ© au moment ou vous accÃ©dez Ã  l'appli, sinon Ã§a ne marchera pas)
**URL publique** : [https://rag-rugby.omar.christianmbip.engineer](https://rag-rugby.omar.christianmbip.engineer)

## ğŸ“‹ Concept de l'application

Cette application est un chatbot intelligent basÃ© sur la technique **RAG (Retrieval-Augmented Generation)** qui rÃ©pond Ã  des questions sur les rÃ¨gles du rugby en franÃ§ais. 

### Fonctionnement
1. **Ingestion** : Le document PDF "regles-du-rugby.pdf" (216 pages) est chargÃ© et divisÃ© en chunks de 1000 caractÃ¨res
2. **Vectorisation** : Chaque chunk est transformÃ© en embeddings avec le modÃ¨le `nomic-embed-text`
3. **Stockage** : Les embeddings sont stockÃ©s dans une base vectorielle Chroma persistante
4. **Recherche** : Lorsqu'une question est posÃ©e, les 3 chunks les plus pertinents sont rÃ©cupÃ©rÃ©s
5. **GÃ©nÃ©ration** : Le modÃ¨le `Llama 3.2` gÃ©nÃ¨re une rÃ©ponse concise basÃ©e sur le contexte rÃ©cupÃ©rÃ©

### CaractÃ©ristiques
- âœ… RÃ©ponses prÃ©cises basÃ©es uniquement sur le document source
- âœ… Interface web intuitive avec Gradio
- âœ… Cache LRU pour optimiser les questions rÃ©pÃ©tÃ©es
- âœ… Nettoyage automatique des balises XML dans les rÃ©ponses
- âœ… DÃ©ploiement avec Docker et tunnel Cloudflare

---

## ğŸš€ Installation et exÃ©cution en local

### PrÃ©requis
- Docker et Docker Compose installÃ©s
- 8 GB de RAM minimum (recommandÃ© : 16 GB)
- Connexion internet pour tÃ©lÃ©charger les modÃ¨les

### Ã‰tapes d'installation

#### 1. Cloner le projet
```bash
git clone https://github.com/Om1-ba/chat-bot-RAG-Rugby-.git
cd chat-bot-RAG-Rugby-
```

#### 2. Structure des fichiers
Assurez-vous d'avoir cette structure :
```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ regles-du-rugby.pdf
```

#### 3. Lancer les conteneurs Docker
```bash
docker-compose up -d
```

#### 4. TÃ©lÃ©charger les modÃ¨les Ollama
**âš ï¸ IMPORTANT** : TÃ©lÃ©chargez les modÃ¨les avant de lancer l'application
```bash
# TÃ©lÃ©charger Llama 3.2 (~2 GB) - peut prendre 5-10 minutes
docker exec -it ollama ollama pull llama3.2

# TÃ©lÃ©charger nomic-embed-text (~274 MB) - environ 1-2 minutes
docker exec -it ollama ollama pull nomic-embed-text
```

**VÃ©rifier que les modÃ¨les sont installÃ©s :**
```bash
docker exec -it ollama ollama list
```

Vous devriez voir :
```
NAME                    ID              SIZE
llama3.2:latest         a80c4f17acd5    2.0 GB
nomic-embed-text:latest 0a109f422b47    274 MB
```

#### 5. RedÃ©marrer l'application RAG
```bash
docker-compose restart rag-app
```

#### 6. AccÃ©der Ã  l'interface
Ouvrez votre navigateur : **http://localhost:7860**

### Commandes utiles
```bash
# Voir les logs en temps rÃ©el
docker-compose logs -f rag-app

# VÃ©rifier l'Ã©tat des conteneurs
docker-compose ps

# RedÃ©marrer uniquement le chatbot
docker-compose restart rag-app

# ArrÃªter tous les conteneurs
docker-compose down

# Supprimer les volumes (rÃ©initialiser la base vectorielle)
docker-compose down -v

# AccÃ©der au shell du conteneur Ollama
docker exec -it ollama bash
```

---

## ğŸ› ï¸ Choix techniques

### Architecture
- **Docker Compose** : Orchestration de deux services (Ollama + Application)
- **Volumes persistants** : 
  - `ollama_data` : Stocke les modÃ¨les tÃ©lÃ©chargÃ©s
  - `chroma_data` : Conserve la base vectorielle entre les redÃ©marrages

### Stack technique
| Composant | Technologie | Justification |
|-----------|-------------|---------------|
| **LLM** | Llama 3.2 | ModÃ¨le lÃ©ger (2GB) avec bon Ã©quilibre qualitÃ©/rapiditÃ© |
| **Embeddings** | nomic-embed-text | OptimisÃ© pour la recherche sÃ©mantique |
| **Vector DB** | ChromaDB | LÃ©ger, simple, avec persistance locale |
| **Framework** | LangChain | Abstraction RAG et intÃ©gration Ollama |
| **Interface** | Gradio | DÃ©ploiement rapide d'UI avec partage public |
| **PDF Parser** | PyMuPDF | Extraction de texte robuste |
| **Tunnel** | Cloudflare Argo | Exposition sÃ©curisÃ©e sans configuration rÃ©seau |

### Optimisations implÃ©mentÃ©es
1. **Cache LRU** (`@lru_cache`) : Ã‰vite de recalculer les rÃ©ponses identiques
2. **RÃ©duction du contexte** : 3 chunks au lieu de 4 (balance pertinence/vitesse)
3. **TempÃ©rature basse** (0.1) : RÃ©ponses plus dÃ©terministes et rapides
4. **Persistance Chroma** : La vectorisation ne se fait qu'une seule fois

---

## âš ï¸ Limitations et amÃ©liorations possibles

### Limitations actuelles
1. **Langue** : Uniquement le franÃ§ais (liÃ© au PDF source)
2. **Monodocument** : L'application ne traite qu'un seul PDF
3. **Pas de streaming** : Les rÃ©ponses apparaissent d'un coup (Gradio ne supporte pas le streaming Ollama nativement)
4. **Ressources** : NÃ©cessite un serveur avec GPU pour de meilleures performances (actuellement CPU)
5. **Pas de mÃ©moire conversationnelle** : Chaque question est traitÃ©e indÃ©pendamment

### AmÃ©liorations possibles
- [ ] Ajouter un systÃ¨me de conversation multi-tours avec historique
- [ ] Supporter le tÃ©lÃ©versement de PDFs personnalisÃ©s
- [ ] ImplÃ©menter un systÃ¨me de feedback utilisateur (ğŸ‘/ğŸ‘)
- [ ] Ajouter des mÃ©tadonnÃ©es (numÃ©ros de page) dans les rÃ©ponses
- [ ] Migrer vers un modÃ¨le avec GPU (ex: via Modal, Runpod)
- [ ] Interface multilingue avec traduction automatique
- [ ] Ajout de graphiques de similaritÃ© des chunks rÃ©cupÃ©rÃ©s

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨me : "Connection refused" lors du lancement
**Solution** : VÃ©rifiez que les modÃ¨les sont tÃ©lÃ©chargÃ©s
```bash
docker exec -it ollama ollama list
```

### ProblÃ¨me : L'application ne trouve pas les modÃ¨les
**Solution** : RedÃ©marrez le conteneur aprÃ¨s tÃ©lÃ©chargement
```bash
docker-compose restart rag-app
```

### ProblÃ¨me : RÃ©ponses trÃ¨s lentes
**Causes possibles** :
- CPU uniquement (pas de GPU)
- RAM insuffisante
- ModÃ¨le trop lourd

**Solution** : RÃ©duire `chunk_size` ou utiliser un modÃ¨le plus petit

---

## ğŸŒ Application dÃ©ployÃ©e (via Cloudflared : mon pc doit donc etre allumÃ© au moment ou vous accÃ©dez Ã  l'appli, sinon Ã§a ne marchera pas)

**URL publique** : [https://rag-rugby.omar.christianmbip.engineer](https://rag-rugby.omar.christianmbip.engineer)

> âš ï¸ **Note** : Le tunnel Cloudflare nÃ©cessite que le service soit actif en permanence. Si l'URL ne rÃ©pond pas, relancez le tunnel avec :
> ```bash
> cloudflared tunnel run rag-rugby
> ```

---

## ğŸ‘¥ Auteurs

**Christian & Omar** | Projet RAG - Guide du Rugby  
AlimentÃ© par Llama 3.2 et Ollama

---

## ğŸ“„ Licence

Ce projet est Ã  usage Ã©ducatif. Le document "regles-du-rugby.pdf" appartient Ã  ses auteurs respectifs.
