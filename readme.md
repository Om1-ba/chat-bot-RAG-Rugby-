# ğŸ‰ RAG Rugby Chatbot (Dockerized)

This project is a **Retrieval-Augmented Generation (RAG) chatbot** built with:

- **LangChain**
- **ChromaDB**
- **Ollama**
- **DeepSeek-R1**
- **Gradio**

It allows you to ask questions about the book **â€œComprendre le Rugbyâ€** using a local LLM.

The project is fully **Dockerized**, so it can run on **any computer** without installing Python or dependencies manually.

---

## ğŸ“ Project Structure


---

## âš™ï¸ Requirements

You only need:

- **Docker**
- **Docker Compose**

### Check installation

```bash
docker --version
docker-compose --version

ğŸš€ How to Launch the Project
1ï¸âƒ£ Clone the repository

git clone <your-repository-url>
cd rag-rugby

2ï¸âƒ£ Build and start all services
docker-compose up --build
This will start:

Ollama (LLM server)

The RAG + Gradio application

ChromaDB (persistent vector store)
3ï¸âƒ£ Download required models (first run only)

Open a new terminal and run:

docker exec -it ollama ollama pull deepseek-r1
docker exec -it ollama ollama pull nomic-embed-text
The models are stored in a Docker volume and will not be downloaded again.

4ï¸âƒ£ Open the application
Once everything is running, open your browser:
http://localhost:7860

You can now ask questions about the rugby book.

ğŸ’¾ Data Persistence

The following data is persisted automatically using Docker volumes:

Ollama models

Chroma vector database

Stopping or restarting containers will not delete embeddings or models.

ğŸ›‘ Stop the Project

To stop all services:

docker-compose down


To stop and remove volumes (âš ï¸ deletes models and embeddings):

docker-compose down -v

ğŸ§  Technologies Used

Python 3.11

LangChain

ChromaDB

Ollama

DeepSeek-R1

Gradio

Docker & Docker Compose

ğŸ“Œ Notes

The PDF file must remain inside the app/ folder.

First startup may be slow due to model downloads.

Works on Windows, macOS, and Linux.

ğŸ‘¥ Authors

Christian

Omar

ğŸ“„ License

This project is for educational and academic use.